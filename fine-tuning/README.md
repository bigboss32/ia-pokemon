# ğŸ® Fine-tuning GPT-2 XL para Descripciones de PokÃ©mon

Este proyecto implementa un fine-tuning del modelo GPT-2 XL utilizando LoRA (Low-Rank Adaptation) para generar descripciones de PokÃ©mon. El modelo se entrena con un dataset de preguntas y respuestas sobre PokÃ©mon.

## ğŸš€ CaracterÃ­sticas

- **Modelo base**: GPT-2 XL (1.5B parÃ¡metros)
- **TÃ©cnica**: LoRA fine-tuning para eficiencia de memoria
- **MÃ©tricas**: Seguimiento completo del entrenamiento con grÃ¡ficos
- **OptimizaciÃ³n**: Configuraciones para GPU con memoria limitada
- **EvaluaciÃ³n**: Pruebas automÃ¡ticas post-entrenamiento

## ğŸ—ï¸ Arquitectura del Sistema

### Arquitectura General
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE DE FINE-TUNING                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ Dataset                                                 â”‚
â”‚  â””â”€â”€ pokemon_descriptions.txt                              â”‚
â”‚       â”œâ”€â”€ Pregunta: Â¿QuÃ© es Pikachu?                      â”‚
â”‚       â””â”€â”€ Respuesta: Pikachu es un PokÃ©mon elÃ©ctrico...   â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  ğŸ”„ Procesamiento                                           â”‚
â”‚  â””â”€â”€ TokenizaciÃ³n con AutoTokenizer                        â”‚
â”‚       â”œâ”€â”€ Padding y truncamiento                           â”‚
â”‚       â”œâ”€â”€ ConversiÃ³n a input_ids                           â”‚
â”‚       â””â”€â”€ GeneraciÃ³n de labels                             â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  ğŸ§  Modelo GPT-2 XL + LoRA                                 â”‚
â”‚  â””â”€â”€ Arquitectura Transformer                              â”‚
â”‚       â”œâ”€â”€ 48 capas de atenciÃ³n                             â”‚
â”‚       â”œâ”€â”€ 1600 dimensiones ocultas                         â”‚
â”‚       â”œâ”€â”€ 25 cabezas de atenciÃ³n                           â”‚
â”‚       â””â”€â”€ Adaptadores LoRA en capas especÃ­ficas            â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  ğŸ“Š MÃ©tricas y Monitoreo                                   â”‚
â”‚  â””â”€â”€ MetricsTracker                                        â”‚
â”‚       â”œâ”€â”€ PÃ©rdida de entrenamiento                         â”‚
â”‚       â”œâ”€â”€ Learning rate                                    â”‚
â”‚       â”œâ”€â”€ GrÃ¡ficos automÃ¡ticos                            â”‚
â”‚       â””â”€â”€ Resumen de rendimiento                           â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  ğŸ’¾ Modelo Entrenado                                       â”‚
â”‚  â””â”€â”€ Adaptadores LoRA + Tokenizer                          â”‚
â”‚       â”œâ”€â”€ adapter_model.bin                                â”‚
â”‚       â”œâ”€â”€ adapter_config.json                              â”‚
â”‚       â””â”€â”€ MÃ©tricas de entrenamiento                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitectura GPT-2 XL
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GPT-2 XL ARCHITECTURE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Especificaciones                                        â”‚
â”‚  â”œâ”€â”€ ParÃ¡metros: 1.5B                                      â”‚
â”‚  â”œâ”€â”€ Capas: 48                                             â”‚
â”‚  â”œâ”€â”€ Dimensiones ocultas: 1600                             â”‚
â”‚  â”œâ”€â”€ Cabezas de atenciÃ³n: 25                               â”‚
â”‚  â”œâ”€â”€ Vocabulario: 50,257 tokens                            â”‚
â”‚  â””â”€â”€ Contexto mÃ¡ximo: 1024 tokens                          â”‚
â”‚                                                             â”‚
â”‚  ğŸ”„ Flujo de datos                                          â”‚
â”‚  Input Text â†’ Tokenizer â†’ Embeddings â†’ Transformer Blocks  â”‚
â”‚                                                             â”‚
â”‚  ğŸ“ Bloque Transformer (x48)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LayerNorm                                           â”‚   â”‚
â”‚  â”‚           â†“                                         â”‚   â”‚
â”‚  â”‚ Multi-Head Attention â† LoRA (c_attn)               â”‚   â”‚
â”‚  â”‚           â†“                                         â”‚   â”‚
â”‚  â”‚ Residual Connection                                 â”‚   â”‚
â”‚  â”‚           â†“                                         â”‚   â”‚
â”‚  â”‚ LayerNorm                                           â”‚   â”‚
â”‚  â”‚           â†“                                         â”‚   â”‚
â”‚  â”‚ Feed Forward (c_fc) â† LoRA                         â”‚   â”‚
â”‚  â”‚           â†“                                         â”‚   â”‚
â”‚  â”‚ Projection (c_proj) â† LoRA                         â”‚   â”‚
â”‚  â”‚           â†“                                         â”‚   â”‚
â”‚  â”‚ Residual Connection                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  ğŸ¯ Cabeza de salida                                        â”‚
â”‚  Final LayerNorm â†’ Linear Layer â†’ Softmax â†’ Probabilidades â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitectura LoRA (Low-Rank Adaptation)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       LoRA ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¯ Concepto                                                â”‚
â”‚  En lugar de entrenar toda la matriz W, LoRA entrena       â”‚
â”‚  dos matrices mÃ¡s pequeÃ±as A y B tal que: W' = W + BA      â”‚
â”‚                                                             â”‚
â”‚  ğŸ“Š ConfiguraciÃ³n                                           â”‚
â”‚  â”œâ”€â”€ Rango (r): 16                                         â”‚
â”‚  â”œâ”€â”€ Alpha: 32                                             â”‚
â”‚  â”œâ”€â”€ Dropout: 0.1                                          â”‚
â”‚  â””â”€â”€ MÃ³dulos objetivo: c_attn, c_proj, c_fc                â”‚
â”‚                                                             â”‚
â”‚  ğŸ”„ Flujo LoRA                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  Input (d)                                          â”‚   â”‚
â”‚  â”‚     â”‚                                               â”‚   â”‚
â”‚  â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚     â”‚                                             â”‚ â”‚   â”‚
â”‚  â”‚     â–¼                                             â”‚ â”‚   â”‚
â”‚  â”‚  Original Weight W                                 â”‚ â”‚   â”‚
â”‚  â”‚  (frozen, no training)                            â”‚ â”‚   â”‚
â”‚  â”‚     â”‚                                             â”‚ â”‚   â”‚
â”‚  â”‚     â”‚                                             â”‚ â”‚   â”‚
â”‚  â”‚     â–¼                                             â”‚ â”‚   â”‚
â”‚  â”‚  LoRA Branch A (d Ã— r)                           â”‚ â”‚   â”‚
â”‚  â”‚     â”‚                                             â”‚ â”‚   â”‚
â”‚  â”‚     â–¼                                             â”‚ â”‚   â”‚
â”‚  â”‚  LoRA Branch B (r Ã— d)                           â”‚ â”‚   â”‚
â”‚  â”‚     â”‚                                             â”‚ â”‚   â”‚
â”‚  â”‚     â–¼                                             â”‚ â”‚   â”‚
â”‚  â”‚  Scale by Î±/r                                    â”‚ â”‚   â”‚
â”‚  â”‚     â”‚                                             â”‚ â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                     â”‚                               â”‚   â”‚
â”‚  â”‚                     â–¼                               â”‚   â”‚
â”‚  â”‚                  Add (+)                            â”‚   â”‚
â”‚  â”‚                     â”‚                               â”‚   â”‚
â”‚  â”‚                     â–¼                               â”‚   â”‚
â”‚  â”‚                  Output                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  ğŸ’¡ Ventajas                                                â”‚
â”‚  â”œâ”€â”€ Solo entrena ~1% de los parÃ¡metros originales         â”‚
â”‚  â”œâ”€â”€ Reduce significativamente el uso de memoria           â”‚
â”‚  â”œâ”€â”€ Permite fine-tuning eficiente                         â”‚
â”‚  â””â”€â”€ Mantiene la calidad del modelo base                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitectura de Entrenamiento
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRAINING ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ”„ Flujo de Entrenamiento                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  Batch de datos                                     â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  Forward Pass                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ TokenizaciÃ³n                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Embeddings                                     â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Transformer Blocks (con LoRA)                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€ PredicciÃ³n de siguiente token                  â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  CÃ¡lculo de pÃ©rdida                                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€ CrossEntropyLoss                               â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  Backward Pass                                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Gradientes solo para parÃ¡metros LoRA           â”‚   â”‚
â”‚  â”‚  â””â”€â”€ ParÃ¡metros base congelados                     â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  ActualizaciÃ³n de parÃ¡metros                       â”‚   â”‚
â”‚  â”‚  â””â”€â”€ AdamW Optimizer                                â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  MÃ©tricas y logging                                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€ MetricsTracker                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  ğŸ“Š Componentes del sistema                                 â”‚
â”‚  â”œâ”€â”€ DataCollatorForLanguageModeling                       â”‚
â”‚  â”‚   â””â”€â”€ Maneja padding y creaciÃ³n de batches             â”‚
â”‚  â”œâ”€â”€ Custom Trainer                                        â”‚
â”‚  â”‚   â””â”€â”€ Extiende Trainer con mÃ©tricas personalizadas     â”‚
â”‚  â”œâ”€â”€ MetricsTracker                                        â”‚
â”‚  â”‚   â””â”€â”€ Registra y visualiza mÃ©tricas                    â”‚
â”‚  â””â”€â”€ Schedulers                                            â”‚
â”‚      â””â”€â”€ Cosine annealing para learning rate               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Arquitectura de Datos
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ“ Estructura de datos                                     â”‚
â”‚  pokemon_descriptions.txt                                   â”‚
â”‚  â”œâ”€â”€ Formato: Bloques separados por ###                    â”‚
â”‚  â”œâ”€â”€ Pregunta: [texto]                                     â”‚
â”‚  â”œâ”€â”€ Respuesta: [texto]                                    â”‚
â”‚  â””â”€â”€ Encoding: UTF-8                                       â”‚
â”‚                                                             â”‚
â”‚  ğŸ”„ Pipeline de procesamiento                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  Texto crudo                                        â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  Parsing (read_qa_txt)                             â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Split por ###                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ ExtracciÃ³n de pregunta                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ ExtracciÃ³n de respuesta                        â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Formato: "Pregunta: X\nRespuesta: Y"          â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  TokenizaciÃ³n                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ AutoTokenizer de GPT-2                         â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Padding: False (dinÃ¡mico)                      â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Truncation: True (max_length=256)              â”‚   â”‚
â”‚  â”‚  â””â”€â”€ Special tokens: AÃ±adidos                       â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  Dataset de Hugging Face                            â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ input_ids: Tokens de entrada                   â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ attention_mask: MÃ¡scara de atenciÃ³n            â”‚   â”‚
â”‚  â”‚  â””â”€â”€ labels: Tokens objetivo (copia de input_ids)   â”‚   â”‚
â”‚  â”‚       â”‚                                             â”‚   â”‚
â”‚  â”‚       â–¼                                             â”‚   â”‚
â”‚  â”‚  DataCollator                                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ Padding dinÃ¡mico a mÃºltiplos de 8              â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ CreaciÃ³n de batches                            â”‚   â”‚
â”‚  â”‚  â””â”€â”€ PreparaciÃ³n para GPU                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requisitos

### Dependencias principales
```bash
pip install transformers
pip install peft
pip install datasets
pip install torch
pip install matplotlib
pip install pandas
```

### Requisitos del sistema
- **GPU**: Recomendado 8GB+ VRAM
- **RAM**: 16GB+ recomendado
- **Almacenamiento**: 10GB+ libres

## ğŸ“ Estructura del proyecto

```
pokemon-finetuning/
â”œâ”€â”€ fine-tuning/
â”‚   â””â”€â”€ pokemon_descriptions.txt    # Dataset de entrenamiento
â”œâ”€â”€ pokemon_gpt2xl/                 # Modelo entrenado (se genera)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â”œâ”€â”€ training_metrics.json
â”‚   â”œâ”€â”€ training_metrics.csv
â”‚   â””â”€â”€ training_metrics.png
â”œâ”€â”€ fine_tuning_script.py           # Script principal
â””â”€â”€ README.md                       # Este archivo
```

## ğŸ¯ Formato del dataset

El archivo `pokemon_descriptions.txt` debe tener el siguiente formato:

```
###
Pregunta: Â¿QuÃ© tipo de PokÃ©mon es Pikachu?
Respuesta: Pikachu es un PokÃ©mon de tipo ElÃ©ctrico conocido por sus mejillas que almacenan electricidad.
###

###
Pregunta: Describe a Charizard
Respuesta: Charizard es un PokÃ©mon de tipo Fuego/Volador con forma de dragÃ³n que puede volar y lanzar llamas.
###
```

## ğŸ”§ ConfiguraciÃ³n

### ParÃ¡metros de LoRA
- `r`: 16 (rango de las matrices de bajo rango)
- `lora_alpha`: 32 (factor de escalamiento)
- `lora_dropout`: 0.1 (dropout para regularizaciÃ³n)
- `target_modules`: ["c_attn", "c_proj", "c_fc"]

### HiperparÃ¡metros de entrenamiento
- `num_train_epochs`: 5
- `per_device_train_batch_size`: 1
- `gradient_accumulation_steps`: 4
- `learning_rate`: 5e-5
- `weight_decay`: 0.01
- `lr_scheduler_type`: "cosine"

## ğŸš€ Uso

### 1. Preparar el dataset
AsegÃºrate de tener el archivo `fine-tuning/pokemon_descriptions.txt` con el formato correcto.

### 2. Ejecutar el entrenamiento
```bash
python fine_tuning_script.py
```

### 3. Monitorear el progreso
El script mostrarÃ¡:
- Uso de memoria GPU
- PÃ©rdida de entrenamiento en tiempo real
- Progreso por Ã©pocas

### 4. Resultados
DespuÃ©s del entrenamiento encontrarÃ¡s:
- Modelo entrenado en `./pokemon_gpt2xl/`
- MÃ©tricas en `training_metrics.json` y `training_metrics.csv`
- GrÃ¡ficos en `training_metrics.png`

## ğŸ“Š MÃ©tricas de entrenamiento

El script genera automÃ¡ticamente:

### GrÃ¡ficos
- **PÃ©rdida por paso**: EvoluciÃ³n de la pÃ©rdida durante el entrenamiento
- **PÃ©rdida por Ã©poca**: Progreso general del modelo
- **Tasa de aprendizaje**: EvoluciÃ³n del learning rate
- **Progreso (escala log)**: Vista logarÃ­tmica del progreso

### Archivos de mÃ©tricas
- **JSON**: Datos completos con timestamps
- **CSV**: Datos tabulares para anÃ¡lisis posterior

### Resumen automÃ¡tico
```
ğŸ“Š RESUMEN DE MÃ‰TRICAS DE ENTRENAMIENTO
==================================================
PÃ©rdida inicial: 3.2450
PÃ©rdida final: 1.8920
PÃ©rdida mÃ­nima: 1.8920
PÃ©rdida mÃ¡xima: 3.2450
Mejora total: 41.72%
Total de pasos: 125
DuraciÃ³n del entrenamiento: 1847.32 segundos
==================================================
```

## ğŸ§ª EvaluaciÃ³n

El script incluye una prueba automÃ¡tica que:
1. Carga el modelo entrenado
2. Genera una respuesta a una pregunta de prueba
3. Muestra el resultado generado

Ejemplo de salida:
```
âœ¨ Resultado: Pregunta: DescrÃ­beme a Pikachu
Respuesta: Pikachu es un PokÃ©mon de tipo ElÃ©ctrico pequeÃ±o y amarillo...
```

## âš™ï¸ Optimizaciones para GPU limitada

### Si tienes problemas de memoria:
1. **Reduce batch_size**: Cambia `per_device_train_batch_size` a 1
2. **Aumenta gradient_accumulation**: Incrementa `gradient_accumulation_steps`
3. **Habilita gradient checkpointing**: `gradient_checkpointing=True`
4. **Usa FP16**: `fp16=True` (en algunas configuraciones)

### Variables de entorno Ãºtiles:
```python
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
```

## ğŸ” Troubleshooting

### Error: CUDA Out of Memory
```bash
# Soluciones:
1. Reduce batch_size a 1
2. Aumenta gradient_accumulation_steps
3. Habilita gradient_checkpointing
4. Cierra otros procesos que usen GPU
```

### Error: Dataset vacÃ­o
```bash
# Verifica:
1. El archivo pokemon_descriptions.txt existe
2. El formato es correcto (###, Pregunta:, Respuesta:)
3. El encoding es UTF-8
```

### Error: Tokenizer pad_token
```bash
# El script maneja esto automÃ¡ticamente:
tokenizer.pad_token = tokenizer.eos_token
```

## ğŸ“ˆ InterpretaciÃ³n de resultados

### MÃ©tricas importantes:
- **PÃ©rdida decreciente**: Indica que el modelo estÃ¡ aprendiendo
- **Mejora > 30%**: Buen progreso en el entrenamiento
- **PÃ©rdida estable**: El modelo ha convergido

### SeÃ±ales de alerta:
- **PÃ©rdida aumenta**: Posible overfitting o learning rate alto
- **PÃ©rdida se estanca**: Modelo no estÃ¡ aprendiendo
- **GeneraciÃ³n repetitiva**: Ajustar temperature o top_k

## ğŸ¤ Contribuciones


Para mejorar el proyecto:
1. Agrega mÃ¡s datos de entrenamiento
2. Experimenta con hiperparÃ¡metros
3. Implementa mÃ©tricas adicionales (BLEU, ROUGE)
4. Agrega validaciÃ³n cruzada

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver LICENSE para mÃ¡s detalles.

## ğŸ™ Reconocimientos

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [PEFT](https://github.com/huggingface/peft)
- [OpenAI GPT-2](https://openai.com/blog/better-language-models/)

---

**Nota**: Este es un proyecto educativo para aprender sobre fine-tuning de modelos de lenguaje. Los resultados pueden variar segÃºn el hardware y los datos utilizados.